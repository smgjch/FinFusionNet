{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_provider import data_factory,data_loader\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1.],\n",
       "       [1., 1.]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [[1,2,3,4],\n",
    "     [4,5,6,7]]\n",
    "np.corrcoef(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 2)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array(a).T\n",
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "args = argparse.Namespace()\n",
    "args.augmentation_ratio = 0    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "testset = data_loader.m4Dataset_btc(args,scale=False,\n",
    "        data_path='btc_t_v_withf4.csv',flag=\"train\",size=[128,0,1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(96, 138)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testset[10][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(96, 1)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testset[0][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def extract_data_from_text(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    blocks = []\n",
    "    current_block = []\n",
    "    for line in lines:\n",
    "        if \"Args in experiment:\" in line and current_block:\n",
    "            blocks.append(current_block)\n",
    "            current_block = []\n",
    "        current_block.append(line)\n",
    "    if current_block:\n",
    "        blocks.append(current_block)\n",
    "\n",
    "    block_names = []\n",
    "    avg_val_ic_values = []\n",
    "    avg_test_ic_values = []\n",
    "    std_val_ic_values = []\n",
    "    std_test_ic_values = []\n",
    "\n",
    "    for block in blocks:\n",
    "        block_text = \"\".join(block)\n",
    "        model_match = re.search(r\"Model:\\s*(\\S+)\", block_text)\n",
    "\n",
    "        val_ic_matches = re.findall(r\"val ic:\\s*([0-9.-]+)\", block_text)\n",
    "        test_ic_matches = re.findall(r\"test ic:\\s*([0-9.-]+)\", block_text)\n",
    "\n",
    "        if model_match and val_ic_matches and test_ic_matches:\n",
    "            block_names.append(model_match.group(1))\n",
    "            val_ic_floats = [float(value) for value in val_ic_matches]\n",
    "            test_ic_floats = [float(value) for value in test_ic_matches]\n",
    "\n",
    "            avg_val_ic_values.append(np.mean(val_ic_floats))\n",
    "            avg_test_ic_values.append(np.mean(test_ic_floats))\n",
    "            std_val_ic_values.append(np.std(val_ic_floats))\n",
    "            std_test_ic_values.append(np.std(test_ic_floats))\n",
    "\n",
    "    return block_names, avg_val_ic_values, avg_test_ic_values, std_val_ic_values, std_test_ic_values\n",
    "\n",
    "def plot_ic_values(block_names, avg_val_ic_values, avg_test_ic_values, std_val_ic_values, std_test_ic_values):\n",
    "    for i, name in enumerate(block_names):\n",
    "        print(f'Block: {name}')\n",
    "        print(f'Average val ic: {avg_val_ic_values[i]}, Std val ic: {std_val_ic_values[i]}')\n",
    "        print(f'Average test ic: {avg_test_ic_values[i]}, Std test ic: {std_test_ic_values[i]}')\n",
    "        print('-' * 50)\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    for i, name in enumerate(block_names):\n",
    "        plt.scatter(i, avg_val_ic_values[i], color='blue', label='val ic' if i == 0 else \"\")\n",
    "        plt.scatter(i, avg_test_ic_values[i], color='red', label='test ic' if i == 0 else \"\")\n",
    "        plt.text(i, avg_val_ic_values[i], f'{name}_val', fontsize=9, ha='right')\n",
    "        plt.text(i, avg_test_ic_values[i], f'{name}_test', fontsize=9, ha='right')\n",
    "\n",
    "    plt.xlabel(\"Block Index\")\n",
    "    plt.ylabel(\"IC Value\")\n",
    "    plt.title(\"Average val ic and test ic values for each block\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "file_path = 'log_overall_arch.txt'  # Replace with the actual file path\n",
    "\n",
    "# block_names, avg_val_ic_values, avg_test_ic_values, std_val_ic_values, std_test_ic_values = extract_data_from_text(file_path)\n",
    "# plot_ic_values(block_names, avg_val_ic_values, avg_test_ic_values, std_val_ic_values, std_test_ic_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.MLPs import Baseline_5,Baseline_10,Baseline_15,Baseline_20,Baseline_25\n",
    "import torch\n",
    "from collections import OrderedDict\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.MultiheadAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "self.encoder_layer = nn.TransformerEncoderLayer(d_model=input_size, nhead=3, dim_feedforward=512)\n",
    "self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_summary(model, *inputs):\n",
    "    def forward_hook(module, input, output):\n",
    "        class_name = str(module.__class__).split(\".\")[-1].split(\"'\")[0]\n",
    "        module_idx = len(summary)\n",
    "\n",
    "        m_key = \"%s-%i\" % (class_name, module_idx + 1)\n",
    "        summary[m_key] = OrderedDict()\n",
    "        summary[m_key][\"input_shape\"] = list(input[0].size())\n",
    "        summary[m_key][\"output_shape\"] = list(output.size())\n",
    "        \n",
    "        params = 0\n",
    "        if hasattr(module, \"weight\") and hasattr(module.weight, \"size\"):\n",
    "            params += torch.prod(torch.LongTensor(list(module.weight.size())))\n",
    "            summary[m_key][\"trainable\"] = module.weight.requires_grad\n",
    "        if hasattr(module, \"bias\") and hasattr(module.bias, \"size\"):\n",
    "            params += torch.prod(torch.LongTensor(list(module.bias.size())))\n",
    "        summary[m_key][\"nb_params\"] = params\n",
    "\n",
    "    def register_hook(module):\n",
    "        if (\n",
    "            not isinstance(module, nn.Sequential)\n",
    "            and not isinstance(module, nn.ModuleList)\n",
    "            and not (module == model)\n",
    "        ):\n",
    "            hooks.append(module.register_forward_hook(forward_hook))\n",
    "\n",
    "    device = next(model.parameters()).device\n",
    "    dtype = next(model.parameters()).dtype\n",
    "\n",
    "    batch_size = inputs[0].size(0)\n",
    "    summary = OrderedDict()\n",
    "    hooks = []\n",
    "\n",
    "    model.apply(register_hook)\n",
    "    model(*inputs)\n",
    "\n",
    "    for h in hooks:\n",
    "        h.remove()\n",
    "\n",
    "    print(\"----------------------------------------------------------------\")\n",
    "    print(\"Layer (type)               Output Shape         Param #\")\n",
    "    print(\"================================================================\")\n",
    "    total_params = 0\n",
    "    for layer in summary:\n",
    "        line_new = \"{:30}  {:25} {:15}\".format(\n",
    "            layer,\n",
    "            str(summary[layer][\"output_shape\"]),\n",
    "            \"{0:,}\".format(summary[layer][\"nb_params\"]),\n",
    "        )\n",
    "        total_params += summary[layer][\"nb_params\"]\n",
    "        print(line_new)\n",
    "\n",
    "    print(\"================================================================\")\n",
    "    print(\"Total params: {0:,}\".format(total_params))\n",
    "    print(\"----------------------------------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "Linear-1                        [1024, 1, 64]             8,896          \n",
      "ReLU-2                          [1024, 1, 64]             0              \n",
      "Linear-3                        [1024, 1, 32]             2,080          \n",
      "ReLU-4                          [1024, 1, 32]             0              \n",
      "Linear-5                        [1024, 1, 16]             528            \n",
      "ReLU-6                          [1024, 1, 16]             0              \n",
      "Linear-7                        [1024, 1, 8]              136            \n",
      "ReLU-8                          [1024, 1, 8]              0              \n",
      "Linear-9                        [1024, 1, 4]              36             \n",
      "ReLU-10                         [1024, 1, 4]              0              \n",
      "Linear-11                       [1024, 1, 2]              10             \n",
      "ReLU-12                         [1024, 1, 2]              0              \n",
      "Linear-13                       [1024, 1, 1]              3              \n",
      "================================================================\n",
      "Total params: 11,689\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "input1 = torch.randn(1024,1, 138)\n",
    "model = Baseline_5.Model()\n",
    "\n",
    "custom_summary(model, input1, 0,0,0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "Linear-1                        [2, 1, 256]               35,584         \n",
      "BatchNorm1d-2                   [2, 256, 1]               512            \n",
      "ReLU-3                          [2, 1, 256]               0              \n",
      "Linear-4                        [2, 1, 518]               133,126        \n",
      "BatchNorm1d-5                   [2, 518, 1]               1,036          \n",
      "ReLU-6                          [2, 1, 518]               0              \n",
      "Linear-7                        [2, 1, 1024]              531,456        \n",
      "BatchNorm1d-8                   [2, 1024, 1]              2,048          \n",
      "ReLU-9                          [2, 1, 1024]              0              \n",
      "Linear-10                       [2, 1, 2048]              2,099,200      \n",
      "BatchNorm1d-11                  [2, 2048, 1]              4,096          \n",
      "ReLU-12                         [2, 1, 2048]              0              \n",
      "Linear-13                       [2, 1, 4096]              8,392,704      \n",
      "BatchNorm1d-14                  [2, 4096, 1]              8,192          \n",
      "ReLU-15                         [2, 1, 4096]              0              \n",
      "Linear-16                       [2, 1, 4096]              569,344        \n",
      "Linear-17                       [2, 1, 8192]              33,562,624     \n",
      "BatchNorm1d-18                  [2, 8192, 1]              16,384         \n",
      "ReLU-19                         [2, 1, 8192]              0              \n",
      "Linear-20                       [2, 1, 16384]             134,234,112    \n",
      "BatchNorm1d-21                  [2, 16384, 1]             32,768         \n",
      "ReLU-22                         [2, 1, 16384]             0              \n",
      "Linear-23                       [2, 1, 8192]              134,225,920    \n",
      "BatchNorm1d-24                  [2, 8192, 1]              16,384         \n",
      "ReLU-25                         [2, 1, 8192]              0              \n",
      "Linear-26                       [2, 1, 4096]              33,558,528     \n",
      "BatchNorm1d-27                  [2, 4096, 1]              8,192          \n",
      "ReLU-28                         [2, 1, 4096]              0              \n",
      "Linear-29                       [2, 1, 2048]              8,390,656      \n",
      "BatchNorm1d-30                  [2, 2048, 1]              4,096          \n",
      "ReLU-31                         [2, 1, 2048]              0              \n",
      "Linear-32                       [2, 1, 2048]              8,390,656      \n",
      "Linear-33                       [2, 1, 1024]              2,098,176      \n",
      "BatchNorm1d-34                  [2, 1024, 1]              2,048          \n",
      "ReLU-35                         [2, 1, 1024]              0              \n",
      "Linear-36                       [2, 1, 518]               530,950        \n",
      "BatchNorm1d-37                  [2, 518, 1]               1,036          \n",
      "ReLU-38                         [2, 1, 518]               0              \n",
      "Linear-39                       [2, 1, 256]               132,864        \n",
      "BatchNorm1d-40                  [2, 256, 1]               512            \n",
      "ReLU-41                         [2, 1, 256]               0              \n",
      "Linear-42                       [2, 1, 128]               32,896         \n",
      "BatchNorm1d-43                  [2, 128, 1]               256            \n",
      "ReLU-44                         [2, 1, 128]               0              \n",
      "Linear-45                       [2, 1, 64]                8,256          \n",
      "BatchNorm1d-46                  [2, 64, 1]                128            \n",
      "ReLU-47                         [2, 1, 64]                0              \n",
      "Linear-48                       [2, 1, 64]                131,136        \n",
      "Linear-49                       [2, 1, 32]                2,080          \n",
      "BatchNorm1d-50                  [2, 32, 1]                64             \n",
      "ReLU-51                         [2, 1, 32]                0              \n",
      "Linear-52                       [2, 1, 16]                528            \n",
      "BatchNorm1d-53                  [2, 16, 1]                32             \n",
      "ReLU-54                         [2, 1, 16]                0              \n",
      "Linear-55                       [2, 1, 8]                 136            \n",
      "BatchNorm1d-56                  [2, 8, 1]                 16             \n",
      "ReLU-57                         [2, 1, 8]                 0              \n",
      "Linear-58                       [2, 1, 4]                 36             \n",
      "BatchNorm1d-59                  [2, 4, 1]                 8              \n",
      "ReLU-60                         [2, 1, 4]                 0              \n",
      "Linear-61                       [2, 1, 2]                 10             \n",
      "BatchNorm1d-62                  [2, 2, 1]                 4              \n",
      "ReLU-63                         [2, 1, 2]                 0              \n",
      "Linear-64                       [2, 1, 2]                 130            \n",
      "Linear-65                       [2, 1, 1]                 3              \n",
      "================================================================\n",
      "Total params: 367,158,923\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "input1 = torch.randn(2,1, 138)\n",
    "model = Baseline_25.Model()\n",
    "\n",
    "custom_summary(model, input1, 0,0,0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerRegressor(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, nhead, num_encoder_layers, dim_feedforward):\n",
    "        super(TransformerRegressor, self).__init__()\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=input_dim, nhead=nhead, dim_feedforward=dim_feedforward)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=num_encoder_layers)\n",
    "        self.regressor = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.transformer_encoder(x)\n",
    "        x = self.regressor(x[-1])  # use the last time step\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson correlation coefficient: [ 1. nan]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Load the CSV file\n",
    "csv_file = '/root/code/FinFusionNet/dataset/btc/btc_t_v_withf4ct.csv'  # Replace with your CSV file path\n",
    "data = pd.read_csv(csv_file)\n",
    "\n",
    "# Extract the 'range5' column\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson correlation coefficient: [ 1.0000000e+00 -6.0130516e-04]\n"
     ]
    }
   ],
   "source": [
    "range5 = data['range5']\n",
    "# Generate a random array of the same length\n",
    "# arbitrary_array = np.full(len(range5), )\n",
    "mean = 0  # Replace with your desired mean\n",
    "std_dev = 1  # Replace with your desired standard deviation\n",
    "normal_array = np.random.normal(mean, std_dev, len(range5))\n",
    "\n",
    "# Calculate the Pearson correlation coefficient\n",
    "correlation_coefficient, _ = np.corrcoef(range5, normal_array)\n",
    "\n",
    "# Print the result\n",
    "print(f'Pearson correlation coefficient: {correlation_coefficient}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TSRanking",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
